# ğŸ  House Price Prediction â€” Machine Learning Project


![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![CatBoost](https://img.shields.io/badge/Model-CatBoost-orange)
![Kaggle](https://img.shields.io/badge/Kaggle-House%20Prices%20Competition-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸ¯ Objectives
- Perform **Exploratory Data Analysis (EDA)** to understand key factors influencing prices.  
- Handle **missing data**, **outliers**, and **categorical variables** effectively.  
- Apply **feature engineering** techniques to improve model accuracy.  
- Train and compare multiple **regression algorithms** (Linear Regression, Random Forest, XGBoost, etc.).  
- Evaluate model performance using **Root Mean Squared Error (RMSE)** and **cross-validation**.

---

## ğŸ§  Tech Stack & Tools
- **Languages:** Python  
- **Libraries:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`, `xgboost`, `lightgbm`  
- **Environment:** Jupyter Notebook / Kaggle Notebook  

---

## ğŸ” Dataset Overview
- **Source:** [Kaggle: House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)  
- **Train Data:** 1460 rows Ã— 81 columns  
- **Test Data:** 1459 rows Ã— 80 columns  
- **Target Variable:** `SalePrice`  

---

## ğŸ“ˆ Approach
1. **Data Cleaning** â€” Handle null values, fix datatypes, remove noise.  
2. **EDA (Exploratory Data Analysis)** â€” Visualize correlations between features and sale price.  
3. **Feature Engineering** â€” Encode categorical variables, create derived features, and normalize data.  
4. **Model Building** â€” Train various regression models (Linear, Ridge, Lasso, RandomForest, XGBoost).  
5. **Model Evaluation** â€” Compare results using RMSE, cross-validation, and learning curves.  
6. **Prediction & Submission** â€” Generate predictions for the test set and create the Kaggle submission file.  

---

